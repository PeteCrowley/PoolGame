{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 08:53:33.450500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# You have to click the restart kernel button after each time you run it otherwise the graphics get really weird\n",
    "# If you want to train the AI faster without graphics, go into the PoolGame.py file and change the GymMiniGame default render_screen value to False\n",
    "\n",
    "import tf_agents.agents\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import greedy_policy, random_tf_policy\n",
    "from tf_agents.agents import DdpgAgent\n",
    "from tf_agents.environments import suite_gym, utils\n",
    "from gym.envs.registration import register\n",
    "from tf_agents.replay_buffers  import tf_uniform_replay_buffer\n",
    "import tensorflow as tf\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import trajectory\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "register(\n",
    "    id='PoolGame-v0',\n",
    "    entry_point='PoolGame:GymMiniGame',\n",
    "    max_episode_steps=300,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/gym/envs/registration.py:595: UserWarning: \u001B[33mWARN: Overriding environment PoolGame-v0\u001B[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    }
   ],
   "source": [
    "gym_env = suite_gym.load(\"PoolGame-v0\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 08:55:23.789462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(gym_env)\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "gamma = 0.995\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-4\n",
    "actor_fully_connected_layer_units = (400, 300)\n",
    "critic_obs_fc_layers=(400,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(300,)\n",
    "# Params for collect\n",
    "initial_collect_steps=990\n",
    "num_iterations=100\n",
    "collect_steps_per_iteration=10\n",
    "replay_buffer_capacity=50_000\n",
    "batch_size = 64\n",
    "\n",
    "# Params for summaries and logging\n",
    "log_interval = 100\n",
    "save_interval = 500\n",
    "\n",
    "\n",
    "ou_stddev=0.2\n",
    "ou_damping=0.15\n",
    "target_update_tau=0.05\n",
    "target_update_period=5\n",
    "dqda_clipping=None\n",
    "td_errors_loss_fn=tf.compat.v1.losses.huber_loss\n",
    "reward_scale_factor=1.0\n",
    "gradient_clipping=None\n",
    "\n",
    "load_saved_model = True     # False if you want to run it from scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "actor_network = tf_agents.agents.ddpg.actor_network.ActorNetwork(tf_env.observation_spec(), tf_env.action_spec(), fc_layer_params=actor_fully_connected_layer_units)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "critic_network = tf_agents.agents.ddpg.critic_network.CriticNetwork((tf_env.observation_spec(), tf_env.action_spec()), joint_fc_layer_params=critic_joint_fc_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tf_agent = DdpgAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    actor_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    train_step_counter=global_step,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    gamma=gamma,\n",
    "    ou_stddev=ou_stddev,\n",
    "    ou_damping=ou_damping,\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    dqda_clipping=dqda_clipping,\n",
    "    td_errors_loss_fn=td_errors_loss_fn,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "\n",
    ")\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
    "collect_policy = tf_agent.collect_policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x12ace9570>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(tf_agent.collect_data_spec,\n",
    "                                                               batch_size=tf_env.batch_size,\n",
    "                                                               max_length=replay_buffer_capacity)\n",
    "\n",
    "# Create Checkpointer\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=\"SaveFiles/train\",\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer, reward_list):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    reward_list.append(next_time_step[1].numpy()[0])\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps, reward_list):\n",
    "    for _ in range(steps):\n",
    "      collect_step(env, policy, buffer, reward_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def save_results(reward_list):\n",
    "    with open(\"SaveFiles/rewards.pickle\", 'wb') as f:\n",
    "        pickle.dump(reward_list, f)\n",
    "\n",
    "def load_results():\n",
    "    with open(\"SaveFiles/rewards.pickle\", 'rb') as f:\n",
    "        reward_list = pickle.load(f)\n",
    "    return reward_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Saved Reward Data\n",
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/23crowley/Documents/Python/PoolGame/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data.\n",
    "\n",
    "if not load_saved_model:\n",
    "    rewards = []\n",
    "    collect_data(tf_env, random_policy, replay_buffer, steps=initial_collect_steps, reward_list=rewards)\n",
    "else:\n",
    "    print(\"Loading Saved Reward Data\")\n",
    "    rewards = load_results()\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "tf_agent.train = common.function(tf_agent.train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_data(tf_env, tf_agent.collect_policy, replay_buffer, collect_steps_per_iteration, rewards)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience).loss\n",
    "\n",
    "    step = len(rewards)\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f'****** step {step} ******: '\n",
    "              f'\\nlast 100 reward = {sum(rewards[max(len(rewards) - 100, 0):])/min(len(rewards), 100)}'\n",
    "              f'\\nlast 1000 reward = {sum(rewards[max(len(rewards) - 1000, 0):])/min(len(rewards), 1000)}'\n",
    "              f'\\nlast 5000 reward = {sum(rewards[max(len(rewards) - 5000, 0):])/min(len(rewards), 5000)}'\n",
    "              f'\\ntotal average reward = {sum(rewards)/len(rewards)}\\n')\n",
    "\n",
    "    if step % save_interval == 0:\n",
    "        train_checkpointer.save(global_step)\n",
    "        save_results(rewards)\n",
    "        print(\"\\nModel Saved\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reward_lumps = [sum(rewards[i*1000:(i+1)*1000])/1000 for i in range(len(rewards)//1000)]\n",
    "plt.plot(range(1000, len(rewards), 1000), reward_lumps)\n",
    "yl = plt.xlabel(\"Number of Shots Taken\")\n",
    "xl = plt.ylabel(\"Success Rate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
